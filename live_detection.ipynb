{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming weights are already formed. Import torch and load the given model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\asubedi/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2023-12-18 Python-3.9.18 torch-2.1.2+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "custom_YOLOv5s summary: 232 layers, 7254609 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2' has no attribute 'downscale'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfrombytes(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m, (display_width, display_height), sct\u001b[38;5;241m.\u001b[39mgrab(display)\u001b[38;5;241m.\u001b[39mrgb)\n\u001b[0;32m     24\u001b[0m screen \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(np\u001b[38;5;241m.\u001b[39marray(img), cv2\u001b[38;5;241m.\u001b[39mCOLOR_RGB2BGR)\n\u001b[1;32m---> 25\u001b[0m downscaled_img \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownscale\u001b[49m(screen, (\u001b[38;5;241m416\u001b[39m,\u001b[38;5;241m416\u001b[39m))\n\u001b[0;32m     26\u001b[0m result \u001b[38;5;241m=\u001b[39m model(downscaled_img, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m416\u001b[39m)\n\u001b[0;32m     27\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVALORANT Object Detection\u001b[39m\u001b[38;5;124m'\u001b[39m, result\u001b[38;5;241m.\u001b[39mrender()[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2' has no attribute 'downscale'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mss\n",
    "from PIL import Image\n",
    "\n",
    "model_filepath = \"./runs/train/yolov5s_results/weights/best.pt\"\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', model_filepath, force_reload=True)  # custom model\n",
    "\n",
    "# Inference\n",
    "#results = model(img)\n",
    "# Results, change the flowing to: results.show()\n",
    "#results.show()\n",
    "\n",
    "sct = mss.mss()\n",
    "\n",
    "main_display_info = sct.monitors[1]\n",
    "display_width = main_display_info[\"width\"]\n",
    "display_height = main_display_info[\"height\"]\n",
    "display = {\"top\": 0, \"left\": 0, \"width\": display_width, \"height\": display_height}\n",
    "\n",
    "while True:\n",
    "    img = Image.frombytes('RGB', (display_width, display_height), sct.grab(display).rgb)\n",
    "    screen = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
    "    downscaled_img = cv2.resize(screen, (416,416))\n",
    "    result = model(downscaled_img, size=416)\n",
    "    cv2.imshow('VALORANT Object Detection', result.render()[0])\n",
    "\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get our initial window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mss.mss() as sct:\n",
    "    main_display_info = sct.monitors[1]\n",
    "    display = {\"top\": 0, \"left\": 0, \"width\": main_display_info[\"width\"], \"height\": main_display_info[\"height\"]}\n",
    "\n",
    "    while True:\n",
    "        screenshot = sct.grab(display)\n",
    "        #convert window screenshot into np array and correct format\n",
    "        frame = np.array(screenshot)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_path = \"temp_frame.png\"\n",
    "        #downscale image from frame so I can use it with my model\n",
    "        image = cv2.resize(frame, (416,416))\n",
    "        \n",
    "\n",
    "        cv2.imwrite(frame_path, frame)\n",
    "\n",
    "        #want to introduce delay so that I actually see bounding boxes\n",
    "        begin = time.time()\n",
    "        \n",
    "        command = [\"python\", \"detect.py\", \"--source\", frame_path]\n",
    "        subprocess.run()\n",
    "\n",
    "        #mark end time for model being used on image\n",
    "        end = time.time()\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow('Object Detection', results)\n",
    "\n",
    "        # Break the loop if 'q' key is pressed\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YOLOv5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
